{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install git+https://github.com/ildoonet/pytorch-gradual-warmup-lr.git\n!git clone https://github.com/haqishen/SIIM-ISIC-Melanoma-Classification-1st-Place-Solution.git\n%cd SIIM-ISIC-Melanoma-Classification-1st-Place-Solution\n!git clone https://github.com/vcadillog/Ensemble-Deep-Learning-Melanoma-Competition-Pytorch    \n!pip install geffnet\n!pip install git+https://github.com/zhanghang1989/ResNeSt\n!pip install pretrainedmodels","metadata":{"execution":{"iopub.status.busy":"2023-03-17T02:48:24.879571Z","iopub.execute_input":"2023-03-17T02:48:24.879860Z","iopub.status.idle":"2023-03-17T02:49:38.427416Z","shell.execute_reply.started":"2023-03-17T02:48:24.879832Z","shell.execute_reply":"2023-03-17T02:49:38.426112Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/ildoonet/pytorch-gradual-warmup-lr.git\n  Cloning https://github.com/ildoonet/pytorch-gradual-warmup-lr.git to /tmp/pip-req-build-ayacbqoq\n  Running command git clone --filter=blob:none --quiet https://github.com/ildoonet/pytorch-gradual-warmup-lr.git /tmp/pip-req-build-ayacbqoq\n  Resolved https://github.com/ildoonet/pytorch-gradual-warmup-lr.git to commit 6b5e8953a80aef5b324104dc0c2e9b8c34d622bd\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: warmup-scheduler\n  Building wheel for warmup-scheduler (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for warmup-scheduler: filename=warmup_scheduler-0.3.2-py3-none-any.whl size=3880 sha256=ab54fe5fdc80b0ce687bff2e6f3e75a45b3a2699b7d7f7851a470f4b191546f4\n  Stored in directory: /tmp/pip-ephem-wheel-cache-_tpl3z0t/wheels/bf/81/52/0e3bc0b645a339f94c76b4dcb8c8b7a5f588a614f5add83b9f\nSuccessfully built warmup-scheduler\nInstalling collected packages: warmup-scheduler\nSuccessfully installed warmup-scheduler-0.3.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCloning into 'SIIM-ISIC-Melanoma-Classification-1st-Place-Solution'...\nremote: Enumerating objects: 39, done.\u001b[K\nremote: Counting objects: 100% (39/39), done.\u001b[K\nremote: Compressing objects: 100% (34/34), done.\u001b[K\nremote: Total 39 (delta 17), reused 19 (delta 4), pack-reused 0\u001b[K\nReceiving objects: 100% (39/39), 269.34 KiB | 3.37 MiB/s, done.\nResolving deltas: 100% (17/17), done.\n/kaggle/working/SIIM-ISIC-Melanoma-Classification-1st-Place-Solution\nCloning into 'Ensemble-Deep-Learning-Melanoma-Competition-Pytorch'...\nremote: Enumerating objects: 82, done.\u001b[K\nremote: Counting objects: 100% (20/20), done.\u001b[K\nremote: Compressing objects: 100% (20/20), done.\u001b[K\nremote: Total 82 (delta 8), reused 6 (delta 0), pack-reused 62\u001b[K\nReceiving objects: 100% (82/82), 388.41 MiB | 33.08 MiB/s, done.\nResolving deltas: 100% (18/18), done.\nUpdating files: 100% (21/21), done.\nCollecting geffnet\n  Downloading geffnet-1.0.2-py3-none-any.whl (40 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.2/40.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: torch>=1.4 in /opt/conda/lib/python3.7/site-packages (from geffnet) (1.13.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from geffnet) (0.14.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.4->geffnet) (4.4.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchvision->geffnet) (2.28.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision->geffnet) (1.21.6)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision->geffnet) (9.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->geffnet) (2022.12.7)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->geffnet) (2.1.1)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->geffnet) (1.26.14)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->geffnet) (3.4)\nInstalling collected packages: geffnet\nSuccessfully installed geffnet-1.0.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting git+https://github.com/zhanghang1989/ResNeSt\n  Cloning https://github.com/zhanghang1989/ResNeSt to /tmp/pip-req-build-1kkpfs90\n  Running command git clone --filter=blob:none --quiet https://github.com/zhanghang1989/ResNeSt /tmp/pip-req-build-1kkpfs90\n  Resolved https://github.com/zhanghang1989/ResNeSt to commit 1dfb3e8867e2ece1c28a65c9db1cded2818a2031\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from resnest==0.0.6b20230317) (1.21.6)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from resnest==0.0.6b20230317) (4.64.1)\nRequirement already satisfied: nose in /opt/conda/lib/python3.7/site-packages (from resnest==0.0.6b20230317) (1.3.7)\nRequirement already satisfied: torch>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from resnest==0.0.6b20230317) (1.13.0)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.7/site-packages (from resnest==0.0.6b20230317) (9.4.0)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from resnest==0.0.6b20230317) (1.7.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from resnest==0.0.6b20230317) (2.28.2)\nCollecting iopath\n  Downloading iopath-0.1.10.tar.gz (42 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting fvcore\n  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.0.0->resnest==0.0.6b20230317) (4.4.0)\nRequirement already satisfied: yacs>=0.1.6 in /opt/conda/lib/python3.7/site-packages (from fvcore->resnest==0.0.6b20230317) (0.1.8)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from fvcore->resnest==0.0.6b20230317) (6.0)\nRequirement already satisfied: termcolor>=1.1 in /opt/conda/lib/python3.7/site-packages (from fvcore->resnest==0.0.6b20230317) (2.2.0)\nRequirement already satisfied: tabulate in /opt/conda/lib/python3.7/site-packages (from fvcore->resnest==0.0.6b20230317) (0.9.0)\nRequirement already satisfied: portalocker in /opt/conda/lib/python3.7/site-packages (from iopath->resnest==0.0.6b20230317) (2.7.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->resnest==0.0.6b20230317) (2022.12.7)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->resnest==0.0.6b20230317) (1.26.14)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->resnest==0.0.6b20230317) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->resnest==0.0.6b20230317) (3.4)\nBuilding wheels for collected packages: resnest, fvcore, iopath\n  Building wheel for resnest (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for resnest: filename=resnest-0.0.6b20230317-py3-none-any.whl size=49027 sha256=b16682f1cab2e3875a643dff73c931d26fe6c632c1572f40373111ad8678a2df\n  Stored in directory: /tmp/pip-ephem-wheel-cache-adq9vphh/wheels/ee/c4/e6/1d074b9a42c390cd3cb2bd81aa752462b5ed39ef6aa9aa47ec\n  Building wheel for fvcore (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61431 sha256=d03ff7b4d1e473a6241b5f7f035ffa93c81b48a07d352850e53da5425c4e6864\n  Stored in directory: /root/.cache/pip/wheels/12/a2/36/21b9bde5f8deeeb6312efe88ddde26a51facbd2089f32917b3\n  Building wheel for iopath (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31547 sha256=46acad4f2b33c6129d102c34273b5a5aeef96581523765bc3cc9fec2fff5312d\n  Stored in directory: /root/.cache/pip/wheels/96/9a/78/61eeeec98da40f44085da9ba3fec952b4ab7224f5c5be75126\nSuccessfully built resnest fvcore iopath\nInstalling collected packages: iopath, fvcore, resnest\nSuccessfully installed fvcore-0.1.5.post20221221 iopath-0.1.10 resnest-0.0.6b20230317\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting pretrainedmodels\n  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from pretrainedmodels) (1.13.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from pretrainedmodels) (0.14.0)\nRequirement already satisfied: munch in /opt/conda/lib/python3.7/site-packages (from pretrainedmodels) (2.5.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from pretrainedmodels) (4.64.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from munch->pretrainedmodels) (1.16.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch->pretrainedmodels) (4.4.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision->pretrainedmodels) (9.4.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchvision->pretrainedmodels) (2.28.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision->pretrainedmodels) (1.21.6)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->pretrainedmodels) (2022.12.7)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->pretrainedmodels) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->pretrainedmodels) (1.26.14)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->pretrainedmodels) (2.1.1)\nBuilding wheels for collected packages: pretrainedmodels\n  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60966 sha256=86722acf02f8e54ef98055a8ec1b4347c8781e5397cdae0c3763ad809c0d7df3\n  Stored in directory: /root/.cache/pip/wheels/4f/89/a3/5cf59e30a8a75c917c313f14da0f6209be2d147e3160b985d6\nSuccessfully built pretrainedmodels\nInstalling collected packages: pretrainedmodels\nSuccessfully installed pretrainedmodels-0.7.4\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport time\nimport random\nimport argparse\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport cv2\nimport PIL.Image\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data.sampler import RandomSampler, SequentialSampler\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom util import GradualWarmupSchedulerV2\n\nfrom dataset import get_df, get_transforms, MelanomaDataset\nfrom models import Effnet_Melanoma, Resnest_Melanoma, Seresnext_Melanoma\n\n\ndef set_seed(seed=0):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n\n\ndef train_epoch(model, loader, optimizer):\n\n    model.train()\n    train_loss = []\n    bar = tqdm(loader)\n    for (data, target) in bar:\n\n        optimizer.zero_grad()\n        \n        if args.use_meta:\n            data, meta = data\n            data, meta, target = data.to(device), meta.to(device), target.to(device)\n            logits = model(data, meta)\n        else:\n            data, target = data.to(device), target.to(device)\n            logits = model(data)        \n        \n        loss = criterion(logits, target)\n\n        if not args.use_amp:\n            loss.backward()\n\n        if args.image_size in [896,576]:\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n        optimizer.step()\n\n        loss_np = loss.detach().cpu().numpy()\n        train_loss.append(loss_np)\n        smooth_loss = sum(train_loss[-100:]) / min(len(train_loss), 100)\n        bar.set_description('loss: %.5f, smth: %.5f' % (loss_np, smooth_loss))\n\n    train_loss = np.mean(train_loss)\n    return train_loss\n\n\ndef get_trans(img, I):\n\n    if I >= 4:\n        img = img.transpose(2, 3)\n    if I % 4 == 0:\n        return img\n    elif I % 4 == 1:\n        return img.flip(2)\n    elif I % 4 == 2:\n        return img.flip(3)\n    elif I % 4 == 3:\n        return img.flip(2).flip(3)\n\n\ndef val_epoch(model, loader, mel_idx, is_ext=None, n_test=1, get_output=False):\n\n    model.eval()\n    val_loss = []\n    LOGITS = []\n    PROBS = []\n    TARGETS = []\n    with torch.no_grad():\n        for (data, target) in tqdm(loader):\n            \n            if args.use_meta:\n                data, meta = data\n                data, meta, target = data.to(device), meta.to(device), target.to(device)\n                logits = torch.zeros((data.shape[0], args.out_dim)).to(device)\n                probs = torch.zeros((data.shape[0], args.out_dim)).to(device)\n                for I in range(n_test):\n                    l = model(get_trans(data, I), meta)\n                    logits += l\n                    probs += l.softmax(1)\n            else:\n                data, target = data.to(device), target.to(device)\n                logits = torch.zeros((data.shape[0], args.out_dim)).to(device)\n                probs = torch.zeros((data.shape[0], args.out_dim)).to(device)\n                for I in range(n_test):\n                    l = model(get_trans(data, I))\n                    logits += l\n                    probs += l.softmax(1)\n            logits /= n_test\n            probs /= n_test\n\n            LOGITS.append(logits.detach().cpu())\n            PROBS.append(probs.detach().cpu())\n            TARGETS.append(target.detach().cpu())\n\n            loss = criterion(logits, target)\n            val_loss.append(loss.detach().cpu().numpy())\n\n    val_loss = np.mean(val_loss)\n    LOGITS = torch.cat(LOGITS).numpy()\n    PROBS = torch.cat(PROBS).numpy()\n    TARGETS = torch.cat(TARGETS).numpy()\n\n    if get_output:\n        return LOGITS, PROBS\n    else:\n        acc = (PROBS.argmax(1) == TARGETS).mean() * 100.\n        auc = roc_auc_score((TARGETS == mel_idx).astype(float), PROBS[:, mel_idx])\n        auc_20 = roc_auc_score((TARGETS[is_ext == 0] == mel_idx).astype(float), PROBS[is_ext == 0, mel_idx])\n        return val_loss, acc, auc, auc_20\n\n\ndef run(fold, df, meta_features, n_meta_features, transforms_train, transforms_val, mel_idx):\n\n    if args.DEBUG:\n        args.n_epochs = 5\n        df_train = df[df['fold'] != fold].sample(args.batch_size * 5)\n        df_valid = df[df['fold'] == fold].sample(args.batch_size * 5)\n    else:\n        df_train = df[df['fold'] != fold]\n        df_valid = df[df['fold'] == fold]\n\n    dataset_train = MelanomaDataset(df_train, 'train', meta_features, transform=transforms_train)\n    dataset_valid = MelanomaDataset(df_valid, 'valid', meta_features, transform=transforms_val)\n    train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=args.batch_size, sampler=RandomSampler(dataset_train), num_workers=args.num_workers)\n    valid_loader = torch.utils.data.DataLoader(dataset_valid, batch_size=args.batch_size, num_workers=args.num_workers)\n\n    model = ModelClass(\n        args.enet_type,\n        n_meta_features=n_meta_features,\n        n_meta_dim=[int(nd) for nd in args.n_meta_dim.split(',')],\n        out_dim=args.out_dim,\n        pretrained=True\n    )\n          \n    model = model.to(device)\n    if args.wandb:\n        wandb.watch(model)\n\n    auc_max = 0.\n    auc_20_max = 0.\n    model_file  = os.path.join(args.model_dir, f'{args.kernel_type}_best_fold{fold}.pth')\n    model_file2 = os.path.join(args.model_dir, f'{args.kernel_type}_best_20_fold{fold}.pth')\n    model_file3 = os.path.join(args.model_dir, f'{args.kernel_type}_final_fold{fold}.pth')\n\n    optimizer = optim.Adam(model.parameters(), lr=args.init_lr)\n\n    if DP:\n        model = nn.DataParallel(model)\n\n    scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, args.n_epochs - 1)\n    scheduler_warmup = GradualWarmupSchedulerV2(optimizer, multiplier=10, total_epoch=1, after_scheduler=scheduler_cosine)\n    \n    print(len(dataset_train), len(dataset_valid))\n\n    for epoch in range(1, args.n_epochs + 1):\n        print(time.ctime(), f'Fold {fold}, Epoch {epoch}')\n\n\n        train_loss = train_epoch(model, train_loader, optimizer)\n        val_loss, acc, auc, auc_20 = val_epoch(model, valid_loader, mel_idx, is_ext=df_valid['is_ext'].values)\n        if args.wandb:\n            wandb.log(\n                {\"Epoch\" : epoch,\n                 \"Fold\" : fold,\n                 \"Train Loss\": train_loss,\n                 \"Validation Loss\": val_loss,\n                 \"Accuracy\": acc\n                })            \n\n        content = time.ctime() + ' ' + f'Fold {fold}, Epoch {epoch}, lr: {optimizer.param_groups[0][\"lr\"]:.7f}, train loss: {train_loss:.5f}, valid loss: {(val_loss):.5f}, acc: {(acc):.4f}, auc: {(auc):.6f}, auc_20: {(auc_20):.6f}.'\n        print(content)\n        with open(os.path.join(args.log_dir, f'log_{args.kernel_type}.txt'), 'a') as appender:\n            appender.write(content + '\\n')\n\n        scheduler_warmup.step()    \n        if epoch==2: scheduler_warmup.step() # bug workaround   \n            \n        if auc > auc_max:\n            print('auc_max ({:.6f} --> {:.6f}). Saving model ...'.format(auc_max, auc))\n            torch.save(model.state_dict(), model_file)\n            auc_max = auc\n        if auc_20 > auc_20_max:\n            print('auc_20_max ({:.6f} --> {:.6f}). Saving model ...'.format(auc_20_max, auc_20))\n            torch.save(model.state_dict(), model_file2)\n            auc_20_max = auc_20\n\n    torch.save(model.state_dict(), model_file3)\n\n\ndef train():\n    \n    df, df_test, meta_features, n_meta_features, mel_idx = get_df(\n        args.kernel_type,\n        args.out_dim,\n        args.data_dir,\n        args.data_folder,\n        args.use_meta\n    )\n\n    transforms_train, transforms_val = get_transforms(args.image_size)\n    \n    print('Data preprocessed')\n\n    folds = [int(i) for i in args.fold.split(',')]\n    for fold in folds:\n        run(fold, df, meta_features, n_meta_features, transforms_train, transforms_val, mel_idx)\n\ndef predict():\n\n    df, df_test, meta_features, n_meta_features, mel_idx = get_df(\n        args.kernel_type,\n        args.out_dim,\n        args.data_dir,\n        args.data_folder,\n        args.use_meta\n    )\n\n    transforms_train, transforms_val = get_transforms(args.image_size)\n\n    if args.DEBUG:\n        df_test = df_test.sample(args.batch_size * 3)\n    dataset_test = MelanomaDataset(df_test, 'test', meta_features, transform=transforms_val)\n    test_loader = torch.utils.data.DataLoader(dataset_test, batch_size=args.batch_size, num_workers=args.num_workers)\n\n    # load model\n    models = []\n    folds = [int(i) for i in args.fold.split(',')]\n    for fold in folds:\n#     for fold in range(5):\n\n        if args.eval == 'best':\n            model_file = os.path.join(args.model_dir, f'{args.kernel_type}_best_fold{fold}.pth')\n        elif args.eval == 'best_20':\n            model_file = os.path.join(args.model_dir, f'{args.kernel_type}_best_20_fold{fold}.pth')\n        if args.eval == 'final':\n            model_file = os.path.join(args.model_dir, f'{args.kernel_type}_final_fold{fold}.pth')\n\n        model = ModelClass(\n            args.enet_type,\n            n_meta_features=n_meta_features,\n            n_meta_dim=[int(nd) for nd in args.n_meta_dim.split(',')],\n            out_dim=args.out_dim\n        )\n        model = model.to(device)\n\n        try:  # single GPU model_file\n            model.load_state_dict(torch.load(model_file), strict=True)\n        except:  # multi GPU model_file\n            state_dict = torch.load(model_file)\n            state_dict = {k[7:] if k.startswith('module.') else k: state_dict[k] for k in state_dict.keys()}\n            model.load_state_dict(state_dict, strict=True)\n        \n        if len(os.environ['CUDA_VISIBLE_DEVICES']) > 1:\n            model = torch.nn.DataParallel(model)\n\n        model.eval()\n        models.append(model)\n\n    # predict\n    PROBS = []\n    with torch.no_grad():\n        for (data) in tqdm(test_loader):\n            if args.use_meta:\n                data, meta = data\n                data, meta = data.to(device), meta.to(device)\n                probs = torch.zeros((data.shape[0], args.out_dim)).to(device)\n                for model in models:\n                    for I in range(args.n_test):\n                        l = model(get_trans(data, I), meta)\n                        probs += l.softmax(1)\n            else:   \n                data = data.to(device)\n                probs = torch.zeros((data.shape[0], args.out_dim)).to(device)\n                for model in models:\n                    for I in range(args.n_test):\n                        l = model(get_trans(data, I))\n                        probs += l.softmax(1)\n\n            probs /= args.n_test\n            probs /= len(models)\n\n            PROBS.append(probs.detach().cpu())\n\n    PROBS = torch.cat(PROBS).numpy()\n\n    # save cvs\n    df_test['target'] = PROBS[:, mel_idx]\n    df_test[['image_name', 'target']].to_csv(os.path.join(args.sub_dir, f'sub_{args.kernel_type}_{args.eval}.csv'), index=False)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-03-17T02:59:34.372705Z","iopub.execute_input":"2023-03-17T02:59:34.373569Z","iopub.status.idle":"2023-03-17T02:59:34.423902Z","shell.execute_reply.started":"2023-03-17T02:59:34.373517Z","shell.execute_reply":"2023-03-17T02:59:34.422804Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class Args():\n    def __init__(self, mode = 'train'):\n        assert mode in ['train','predict']\n        self.kernel_type = 'newfold'\n        self.data_dir = '/kaggle/input/'\n        self.data_folder = 256\n        self.image_size = 256\n        self.enet_type = 'efficientnet_b3'\n        self.batch_size = 64\n        self.num_workers = 2\n        self.init_lr = 3e-5\n        self.out_dim = 9\n        self.n_epochs = 15\n        self.use_amp = False\n        self.use_meta = True\n        self.DEBUG = False #True to test the code\n        \n        self.model_dir = './Ensemble-Deep-Learning-Melanoma-Competition-Pytorch/weights/weights_nometa'  #Work in github folder\n#         self.model_dir = './weights'        \n        self.log_dir = './logs'\n        self.CUDA_VISIBLE_DEVICES = '0,1'\n#         self.fold = '0,1,2,3,4'\n        self.fold = '0,1' # for demonstration purposes only trained on 2 folds       #    \n        self.wandb = False # Turn True if you wanna use your W&B account\n        if mode == 'predict':            \n            self.eval = 'best'\n            self.sub_dir = './Ensemble-Deep-Learning-Melanoma-Competition-Pytorch/subs/subs_nometa'   #Saved model in github folder        \n#             self.sub_dir = './subs'\n            self.n_test = 8\n#             self.DEBUG = False # \n        self.n_meta_dim = '512,128'\n    \n","metadata":{"execution":{"iopub.status.busy":"2023-03-17T02:59:40.480497Z","iopub.execute_input":"2023-03-17T02:59:40.481451Z","iopub.status.idle":"2023-03-17T02:59:40.491438Z","shell.execute_reply.started":"2023-03-17T02:59:40.481395Z","shell.execute_reply":"2023-03-17T02:59:40.490214Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2023-03-17T02:50:32.516131Z","iopub.execute_input":"2023-03-17T02:50:32.516684Z","iopub.status.idle":"2023-03-17T02:50:32.630150Z","shell.execute_reply.started":"2023-03-17T02:50:32.516650Z","shell.execute_reply":"2023-03-17T02:50:32.628577Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import wandb\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\n\n# My API token is saved as a label named \"wandb_api\". \n# I strongly suggest you to not copy your API tokens directly in your notebook\nwandb_api = user_secrets.get_secret('wandb_api') \n\nwandb.login(key=wandb_api)\nwandb.init(name='Ensemble')","metadata":{"execution":{"iopub.status.busy":"2023-03-15T12:39:56.337795Z","iopub.execute_input":"2023-03-15T12:39:56.338788Z","iopub.status.idle":"2023-03-15T12:39:59.270380Z","shell.execute_reply.started":"2023-03-15T12:39:56.338736Z","shell.execute_reply":"2023-03-15T12:39:59.269276Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"#TRAINING ENSEMBLE        \nargs = Args('train')\n\nos.makedirs(args.model_dir, exist_ok=True)\nos.makedirs(args.log_dir, exist_ok=True)\nos.environ['CUDA_VISIBLE_DEVICES'] = args.CUDA_VISIBLE_DEVICES\n\nif args.enet_type == 'resnest101':\n    ModelClass = Resnest_Melanoma\nelif args.enet_type == 'seresnext101':\n    ModelClass = Seresnext_Melanoma\nelif 'efficientnet' in args.enet_type:\n    ModelClass = Effnet_Melanoma\nelse:\n    raise NotImplementedError()\n\nDP = len(os.environ['CUDA_VISIBLE_DEVICES']) > 1\n\nset_seed()\n\ncriterion = nn.CrossEntropyLoss()\n\ntrain()\n","metadata":{"execution":{"iopub.status.busy":"2023-03-15T01:23:08.094383Z","iopub.execute_input":"2023-03-15T01:23:08.096956Z","iopub.status.idle":"2023-03-15T11:50:50.506880Z","shell.execute_reply.started":"2023-03-15T01:23:08.096916Z","shell.execute_reply":"2023-03-15T11:50:50.505766Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"100%|██████████| 57964/57964 [02:40<00:00, 361.85it/s]\n100%|██████████| 10982/10982 [00:35<00:00, 307.06it/s]\n/opt/conda/lib/python3.7/site-packages/albumentations/augmentations/transforms.py:1151: FutureWarning: This class has been deprecated. Please use RandomBrightnessContrast\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/albumentations/augmentations/transforms.py:1177: FutureWarning: RandomContrast has been deprecated. Please use RandomBrightnessContrast\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/albumentations/augmentations/blur/transforms.py:185: UserWarning: blur_limit and sigma_limit minimum value can not be both equal to 0. blur_limit minimum value changed to 3.\n  \"blur_limit and sigma_limit minimum value can not be both equal to 0. \"\n/opt/conda/lib/python3.7/site-packages/albumentations/augmentations/dropout/cutout.py:51: FutureWarning: Cutout has been deprecated. Please use CoarseDropout\n  FutureWarning,\n","output_type":"stream"},{"name":"stdout","text":"Data preprocessed\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/efficientnet_b3_ra2-cf984f9c.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b3_ra2-cf984f9c.pth\n","output_type":"stream"},{"name":"stdout","text":"46382 11582\nWed Mar 15 01:26:31 2023 Fold 0, Epoch 1\n","output_type":"stream"},{"name":"stderr","text":"loss: 0.71287, smth: 0.67268: 100%|██████████| 725/725 [20:55<00:00,  1.73s/it]\n100%|██████████| 181/181 [01:34<00:00,  1.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Wed Mar 15 01:49:00 2023 Fold 0, Epoch 1, lr: 0.0000300, train loss: 0.90480, valid loss: 0.59275, acc: 81.2640, auc: 0.894630, auc_20: 0.816913.\nauc_max (0.000000 --> 0.894630). Saving model ...\nauc_20_max (0.000000 --> 0.816913). Saving model ...\nWed Mar 15 01:49:01 2023 Fold 0, Epoch 2\n","output_type":"stream"},{"name":"stderr","text":"loss: 0.35761, smth: 0.49008: 100%|██████████| 725/725 [19:36<00:00,  1.62s/it]\n100%|██████████| 181/181 [01:07<00:00,  2.68it/s]\n/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:1360: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n  \"please use `get_last_lr()`.\", UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"Wed Mar 15 02:09:45 2023 Fold 0, Epoch 2, lr: 0.0003000, train loss: 0.55009, valid loss: 0.43767, acc: 85.4688, auc: 0.913036, auc_20: 0.833126.\nauc_max (0.894630 --> 0.913036). Saving model ...\nauc_20_max (0.816913 --> 0.833126). Saving model ...\nWed Mar 15 02:09:46 2023 Fold 0, Epoch 3\n","output_type":"stream"},{"name":"stderr","text":"loss: 0.37023, smth: 0.43930: 100%|██████████| 725/725 [19:45<00:00,  1.64s/it]\n100%|██████████| 181/181 [01:07<00:00,  2.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"Wed Mar 15 02:30:38 2023 Fold 0, Epoch 3, lr: 0.0002962, train loss: 0.46669, valid loss: 0.40012, acc: 86.8417, auc: 0.925955, auc_20: 0.811977.\nauc_max (0.913036 --> 0.925955). Saving model ...\nWed Mar 15 02:30:39 2023 Fold 0, Epoch 4\n","output_type":"stream"},{"name":"stderr","text":"loss: 0.36529, smth: 0.43360: 100%|██████████| 725/725 [19:58<00:00,  1.65s/it]\n100%|██████████| 181/181 [01:07<00:00,  2.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Wed Mar 15 02:51:45 2023 Fold 0, Epoch 4, lr: 0.0002851, train loss: 0.43096, valid loss: 0.37095, acc: 87.3511, auc: 0.939074, auc_20: 0.887174.\nauc_max (0.925955 --> 0.939074). Saving model ...\nauc_20_max (0.833126 --> 0.887174). Saving model ...\nWed Mar 15 02:51:45 2023 Fold 0, Epoch 5\n","output_type":"stream"},{"name":"stderr","text":"loss: 0.18069, smth: 0.40571: 100%|██████████| 725/725 [20:07<00:00,  1.67s/it]\n100%|██████████| 181/181 [01:08<00:00,  2.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"Wed Mar 15 03:13:02 2023 Fold 0, Epoch 5, lr: 0.0002673, train loss: 0.40281, valid loss: 0.36443, acc: 87.6360, auc: 0.939573, auc_20: 0.865116.\nauc_max (0.939074 --> 0.939573). Saving model ...\nWed Mar 15 03:13:02 2023 Fold 0, Epoch 6\n","output_type":"stream"},{"name":"stderr","text":"loss: 0.34742, smth: 0.36449: 100%|██████████| 725/725 [20:13<00:00,  1.67s/it]\n100%|██████████| 181/181 [01:09<00:00,  2.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"Wed Mar 15 03:34:25 2023 Fold 0, Epoch 6, lr: 0.0002435, train loss: 0.37788, valid loss: 0.35271, acc: 87.8518, auc: 0.944872, auc_20: 0.905957.\nauc_max (0.939573 --> 0.944872). Saving model ...\nauc_20_max (0.887174 --> 0.905957). Saving model ...\nWed Mar 15 03:34:25 2023 Fold 0, Epoch 7\n","output_type":"stream"},{"name":"stderr","text":"loss: 0.30334, smth: 0.34941: 100%|██████████| 725/725 [20:01<00:00,  1.66s/it]\n100%|██████████| 181/181 [01:09<00:00,  2.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"Wed Mar 15 03:55:35 2023 Fold 0, Epoch 7, lr: 0.0002151, train loss: 0.35397, valid loss: 0.34315, acc: 88.2835, auc: 0.949479, auc_20: 0.910778.\nauc_max (0.944872 --> 0.949479). Saving model ...\nauc_20_max (0.905957 --> 0.910778). Saving model ...\nWed Mar 15 03:55:36 2023 Fold 0, Epoch 8\n","output_type":"stream"},{"name":"stderr","text":"loss: 0.20627, smth: 0.33367: 100%|██████████| 725/725 [19:41<00:00,  1.63s/it]\n100%|██████████| 181/181 [01:07<00:00,  2.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Wed Mar 15 04:16:25 2023 Fold 0, Epoch 8, lr: 0.0001834, train loss: 0.33069, valid loss: 0.32881, acc: 89.1729, auc: 0.952030, auc_20: 0.894643.\nauc_max (0.949479 --> 0.952030). Saving model ...\nWed Mar 15 04:16:25 2023 Fold 0, Epoch 9\n","output_type":"stream"},{"name":"stderr","text":"loss: 0.18514, smth: 0.30041: 100%|██████████| 725/725 [19:59<00:00,  1.65s/it]\n100%|██████████| 181/181 [01:08<00:00,  2.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Wed Mar 15 04:37:33 2023 Fold 0, Epoch 9, lr: 0.0001500, train loss: 0.30861, valid loss: 0.31684, acc: 89.1815, auc: 0.958117, auc_20: 0.927402.\nauc_max (0.952030 --> 0.958117). Saving model ...\nauc_20_max (0.910778 --> 0.927402). Saving model ...\nWed Mar 15 04:37:33 2023 Fold 0, Epoch 10\n","output_type":"stream"},{"name":"stderr","text":"loss: 0.26540, smth: 0.28347: 100%|██████████| 725/725 [19:57<00:00,  1.65s/it]\n100%|██████████| 181/181 [01:07<00:00,  2.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Wed Mar 15 04:58:38 2023 Fold 0, Epoch 10, lr: 0.0001166, train loss: 0.27939, valid loss: 0.31670, acc: 89.2937, auc: 0.956531, auc_20: 0.918662.\nWed Mar 15 04:58:38 2023 Fold 0, Epoch 11\n","output_type":"stream"},{"name":"stderr","text":"loss: 0.34047, smth: 0.24623: 100%|██████████| 725/725 [20:04<00:00,  1.66s/it]\n100%|██████████| 181/181 [01:08<00:00,  2.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"Wed Mar 15 05:19:52 2023 Fold 0, Epoch 11, lr: 0.0000849, train loss: 0.26334, valid loss: 0.31410, acc: 89.9154, auc: 0.958825, auc_20: 0.920758.\nauc_max (0.958117 --> 0.958825). Saving model ...\nWed Mar 15 05:19:52 2023 Fold 0, Epoch 12\n","output_type":"stream"},{"name":"stderr","text":"loss: 0.25097, smth: 0.24851: 100%|██████████| 725/725 [20:05<00:00,  1.66s/it]\n100%|██████████| 181/181 [01:07<00:00,  2.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Wed Mar 15 05:41:05 2023 Fold 0, Epoch 12, lr: 0.0000565, train loss: 0.24755, valid loss: 0.31143, acc: 89.8463, auc: 0.959417, auc_20: 0.911490.\nauc_max (0.958825 --> 0.959417). Saving model ...\nWed Mar 15 05:41:05 2023 Fold 0, Epoch 13\n","output_type":"stream"},{"name":"stderr","text":"loss: 0.26929, smth: 0.23892: 100%|██████████| 725/725 [19:31<00:00,  1.62s/it]\n100%|██████████| 181/181 [01:06<00:00,  2.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Wed Mar 15 06:01:43 2023 Fold 0, Epoch 13, lr: 0.0000327, train loss: 0.23357, valid loss: 0.31231, acc: 89.9327, auc: 0.959999, auc_20: 0.919665.\nauc_max (0.959417 --> 0.959999). Saving model ...\nWed Mar 15 06:01:44 2023 Fold 0, Epoch 14\n","output_type":"stream"},{"name":"stderr","text":"loss: 0.22577, smth: 0.21799: 100%|██████████| 725/725 [19:26<00:00,  1.61s/it]\n100%|██████████| 181/181 [01:07<00:00,  2.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Wed Mar 15 06:22:18 2023 Fold 0, Epoch 14, lr: 0.0000149, train loss: 0.22616, valid loss: 0.30876, acc: 89.9672, auc: 0.961439, auc_20: 0.921137.\nauc_max (0.959999 --> 0.961439). Saving model ...\nWed Mar 15 06:22:18 2023 Fold 0, Epoch 15\n","output_type":"stream"},{"name":"stderr","text":"loss: 0.26128, smth: 0.23153: 100%|██████████| 725/725 [19:38<00:00,  1.63s/it]\n100%|██████████| 181/181 [01:08<00:00,  2.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Wed Mar 15 06:43:05 2023 Fold 0, Epoch 15, lr: 0.0000038, train loss: 0.21915, valid loss: 0.30394, acc: 90.0363, auc: 0.961717, auc_20: 0.921845.\nauc_max (0.961439 --> 0.961717). Saving model ...\n46375 11589\nWed Mar 15 06:43:06 2023 Fold 1, Epoch 1\n","output_type":"stream"},{"name":"stderr","text":"loss: 0.64889, smth: 0.68978: 100%|██████████| 725/725 [19:08<00:00,  1.58s/it]\n100%|██████████| 182/182 [01:08<00:00,  2.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Wed Mar 15 07:03:23 2023 Fold 1, Epoch 1, lr: 0.0000300, train loss: 0.93492, valid loss: 0.60616, acc: 80.5850, auc: 0.902445, auc_20: 0.873909.\nauc_max (0.000000 --> 0.902445). Saving model ...\nauc_20_max (0.000000 --> 0.873909). Saving model ...\nWed Mar 15 07:03:23 2023 Fold 1, Epoch 2\n","output_type":"stream"},{"name":"stderr","text":"loss: 0.59599, smth: 0.51587: 100%|██████████| 725/725 [19:27<00:00,  1.61s/it]\n100%|██████████| 182/182 [01:08<00:00,  2.65it/s]\n/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:1360: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n  \"please use `get_last_lr()`.\", UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"Wed Mar 15 07:23:59 2023 Fold 1, Epoch 2, lr: 0.0003000, train loss: 0.55728, valid loss: 0.43255, acc: 85.7020, auc: 0.926573, auc_20: 0.873874.\nauc_max (0.902445 --> 0.926573). Saving model ...\nWed Mar 15 07:24:00 2023 Fold 1, Epoch 3\n","output_type":"stream"},{"name":"stderr","text":"loss: 0.45472, smth: 0.44583: 100%|██████████| 725/725 [19:39<00:00,  1.63s/it]\n100%|██████████| 182/182 [01:06<00:00,  2.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Wed Mar 15 07:44:46 2023 Fold 1, Epoch 3, lr: 0.0002962, train loss: 0.47315, valid loss: 0.40108, acc: 86.3664, auc: 0.933378, auc_20: 0.885233.\nauc_max (0.926573 --> 0.933378). Saving model ...\nauc_20_max (0.873909 --> 0.885233). Saving model ...\nWed Mar 15 07:44:46 2023 Fold 1, Epoch 4\n","output_type":"stream"},{"name":"stderr","text":"loss: 0.45330, smth: 0.43665: 100%|██████████| 725/725 [19:12<00:00,  1.59s/it]\n100%|██████████| 182/182 [01:06<00:00,  2.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Wed Mar 15 08:05:05 2023 Fold 1, Epoch 4, lr: 0.0002851, train loss: 0.43411, valid loss: 0.38367, acc: 86.8410, auc: 0.939463, auc_20: 0.912691.\nauc_max (0.933378 --> 0.939463). Saving model ...\nauc_20_max (0.885233 --> 0.912691). Saving model ...\nWed Mar 15 08:05:05 2023 Fold 1, Epoch 5\n","output_type":"stream"},{"name":"stderr","text":"loss: 0.35910, smth: 0.41674: 100%|██████████| 725/725 [19:25<00:00,  1.61s/it]\n100%|██████████| 182/182 [01:07<00:00,  2.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"Wed Mar 15 08:25:38 2023 Fold 1, Epoch 5, lr: 0.0002673, train loss: 0.40823, valid loss: 0.35827, acc: 87.8764, auc: 0.949008, auc_20: 0.918173.\nauc_max (0.939463 --> 0.949008). Saving model ...\nauc_20_max (0.912691 --> 0.918173). Saving model ...\nWed Mar 15 08:25:39 2023 Fold 1, Epoch 6\n","output_type":"stream"},{"name":"stderr","text":"loss: 0.95663, smth: 0.37831: 100%|██████████| 725/725 [19:32<00:00,  1.62s/it]\n100%|██████████| 182/182 [01:06<00:00,  2.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Wed Mar 15 08:46:17 2023 Fold 1, Epoch 6, lr: 0.0002435, train loss: 0.38314, valid loss: 0.34062, acc: 88.5667, auc: 0.952733, auc_20: 0.925168.\nauc_max (0.949008 --> 0.952733). Saving model ...\nauc_20_max (0.918173 --> 0.925168). Saving model ...\nWed Mar 15 08:46:18 2023 Fold 1, Epoch 7\n","output_type":"stream"},{"name":"stderr","text":"loss: 0.33288, smth: 0.34586: 100%|██████████| 725/725 [19:15<00:00,  1.59s/it]\n100%|██████████| 182/182 [01:06<00:00,  2.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Wed Mar 15 09:06:41 2023 Fold 1, Epoch 7, lr: 0.0002151, train loss: 0.35602, valid loss: 0.33385, acc: 88.8946, auc: 0.952197, auc_20: 0.918566.\nWed Mar 15 09:06:41 2023 Fold 1, Epoch 8\n","output_type":"stream"},{"name":"stderr","text":"loss: 0.27013, smth: 0.32579: 100%|██████████| 725/725 [19:27<00:00,  1.61s/it]\n100%|██████████| 182/182 [01:07<00:00,  2.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"Wed Mar 15 09:27:16 2023 Fold 1, Epoch 8, lr: 0.0001834, train loss: 0.33371, valid loss: 0.32715, acc: 89.3347, auc: 0.955964, auc_20: 0.924154.\nauc_max (0.952733 --> 0.955964). Saving model ...\nWed Mar 15 09:27:16 2023 Fold 1, Epoch 9\n","output_type":"stream"},{"name":"stderr","text":"loss: 0.19520, smth: 0.30348: 100%|██████████| 725/725 [19:35<00:00,  1.62s/it]\n100%|██████████| 182/182 [01:07<00:00,  2.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"Wed Mar 15 09:47:58 2023 Fold 1, Epoch 9, lr: 0.0001500, train loss: 0.31207, valid loss: 0.30986, acc: 89.4900, auc: 0.958777, auc_20: 0.922246.\nauc_max (0.955964 --> 0.958777). Saving model ...\nWed Mar 15 09:47:58 2023 Fold 1, Epoch 10\n","output_type":"stream"},{"name":"stderr","text":"loss: 0.36540, smth: 0.27895: 100%|██████████| 725/725 [19:11<00:00,  1.59s/it]\n100%|██████████| 182/182 [01:06<00:00,  2.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Wed Mar 15 10:08:17 2023 Fold 1, Epoch 10, lr: 0.0001166, train loss: 0.28662, valid loss: 0.31828, acc: 89.9905, auc: 0.957282, auc_20: 0.911481.\nWed Mar 15 10:08:17 2023 Fold 1, Epoch 11\n","output_type":"stream"},{"name":"stderr","text":"loss: 0.12426, smth: 0.26156: 100%|██████████| 725/725 [19:27<00:00,  1.61s/it]\n100%|██████████| 182/182 [01:06<00:00,  2.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Wed Mar 15 10:28:51 2023 Fold 1, Epoch 11, lr: 0.0000849, train loss: 0.26868, valid loss: 0.30742, acc: 89.9733, auc: 0.960906, auc_20: 0.919938.\nauc_max (0.958777 --> 0.960906). Saving model ...\nWed Mar 15 10:28:51 2023 Fold 1, Epoch 12\n","output_type":"stream"},{"name":"stderr","text":"loss: 0.53506, smth: 0.26658: 100%|██████████| 725/725 [19:19<00:00,  1.60s/it]\n100%|██████████| 182/182 [01:06<00:00,  2.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Wed Mar 15 10:49:17 2023 Fold 1, Epoch 12, lr: 0.0000565, train loss: 0.25294, valid loss: 0.30149, acc: 90.5169, auc: 0.962204, auc_20: 0.916018.\nauc_max (0.960906 --> 0.962204). Saving model ...\nWed Mar 15 10:49:17 2023 Fold 1, Epoch 13\n","output_type":"stream"},{"name":"stderr","text":"loss: 0.33624, smth: 0.23248: 100%|██████████| 725/725 [19:09<00:00,  1.59s/it]\n100%|██████████| 182/182 [01:07<00:00,  2.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"Wed Mar 15 11:09:33 2023 Fold 1, Epoch 13, lr: 0.0000327, train loss: 0.23909, valid loss: 0.29744, acc: 90.5686, auc: 0.964711, auc_20: 0.923313.\nauc_max (0.962204 --> 0.964711). Saving model ...\nWed Mar 15 11:09:34 2023 Fold 1, Epoch 14\n","output_type":"stream"},{"name":"stderr","text":"loss: 0.20381, smth: 0.22446: 100%|██████████| 725/725 [19:28<00:00,  1.61s/it]\n100%|██████████| 182/182 [01:07<00:00,  2.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"Wed Mar 15 11:30:09 2023 Fold 1, Epoch 14, lr: 0.0000149, train loss: 0.22847, valid loss: 0.30161, acc: 90.6808, auc: 0.964675, auc_20: 0.924440.\nWed Mar 15 11:30:09 2023 Fold 1, Epoch 15\n","output_type":"stream"},{"name":"stderr","text":"loss: 0.21444, smth: 0.22405: 100%|██████████| 725/725 [19:33<00:00,  1.62s/it]\n100%|██████████| 182/182 [01:07<00:00,  2.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Wed Mar 15 11:50:50 2023 Fold 1, Epoch 15, lr: 0.0000038, train loss: 0.22321, valid loss: 0.30123, acc: 90.5341, auc: 0.964595, auc_20: 0.923668.\n","output_type":"stream"}]},{"cell_type":"code","source":"#PREDICT ENSEMBLE\nargs = Args('predict') \n\nos.makedirs(args.sub_dir, exist_ok=True)\nos.environ['CUDA_VISIBLE_DEVICES'] = args.CUDA_VISIBLE_DEVICES\n\nif args.enet_type == 'resnest101':\n    ModelClass = Resnest_Melanoma\nelif args.enet_type == 'seresnext101':\n    ModelClass = Seresnext_Melanoma\nelif 'efficientnet' in args.enet_type:\n    ModelClass = Effnet_Melanoma\nelse:\n    raise NotImplementedError()\n\nDP = len(os.environ['CUDA_VISIBLE_DEVICES']) > 1\n\npredict()    ","metadata":{"execution":{"iopub.status.busy":"2023-03-15T12:11:19.741191Z","iopub.execute_input":"2023-03-15T12:11:19.741945Z","iopub.status.idle":"2023-03-15T12:19:46.489126Z","shell.execute_reply.started":"2023-03-15T12:11:19.741905Z","shell.execute_reply":"2023-03-15T12:19:46.487841Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stderr","text":"100%|██████████| 57964/57964 [01:58<00:00, 489.75it/s]\n100%|██████████| 10982/10982 [00:25<00:00, 434.86it/s]\n/opt/conda/lib/python3.7/site-packages/albumentations/augmentations/transforms.py:1151: FutureWarning: This class has been deprecated. Please use RandomBrightnessContrast\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/albumentations/augmentations/transforms.py:1177: FutureWarning: RandomContrast has been deprecated. Please use RandomBrightnessContrast\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/albumentations/augmentations/blur/transforms.py:185: UserWarning: blur_limit and sigma_limit minimum value can not be both equal to 0. blur_limit minimum value changed to 3.\n  \"blur_limit and sigma_limit minimum value can not be both equal to 0. \"\n/opt/conda/lib/python3.7/site-packages/albumentations/augmentations/dropout/cutout.py:51: FutureWarning: Cutout has been deprecated. Please use CoarseDropout\n  FutureWarning,\n100%|██████████| 172/172 [06:01<00:00,  2.10s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"wandb.init(name='NoMeta')","metadata":{"execution":{"iopub.status.busy":"2023-03-15T12:40:11.647172Z","iopub.execute_input":"2023-03-15T12:40:11.648171Z","iopub.status.idle":"2023-03-15T12:40:42.418969Z","shell.execute_reply.started":"2023-03-15T12:40:11.648123Z","shell.execute_reply":"2023-03-15T12:40:42.417886Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvcadillo\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.14.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.13.10"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/SIIM-ISIC-Melanoma-Classification-1st-Place-Solution/wandb/run-20230315_124011-kxn8vf5i</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/vcadillo/SIIM-ISIC-Melanoma-Classification-1st-Place-Solution/runs/kxn8vf5i' target=\"_blank\">NoMeta</a></strong> to <a href='https://wandb.ai/vcadillo/SIIM-ISIC-Melanoma-Classification-1st-Place-Solution' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/vcadillo/SIIM-ISIC-Melanoma-Classification-1st-Place-Solution' target=\"_blank\">https://wandb.ai/vcadillo/SIIM-ISIC-Melanoma-Classification-1st-Place-Solution</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/vcadillo/SIIM-ISIC-Melanoma-Classification-1st-Place-Solution/runs/kxn8vf5i' target=\"_blank\">https://wandb.ai/vcadillo/SIIM-ISIC-Melanoma-Classification-1st-Place-Solution/runs/kxn8vf5i</a>"},"metadata":{}},{"execution_count":8,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/vcadillo/SIIM-ISIC-Melanoma-Classification-1st-Place-Solution/runs/kxn8vf5i?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x7fbaf0f8a490>"},"metadata":{}}]},{"cell_type":"code","source":"args = Args('predict')\nargs.use_meta = False\nargs.model_dir = './Ensemble-Deep-Learning-Melanoma-Competition-Pytorch/weights/weights_nometa'\n\n\nos.makedirs(args.model_dir, exist_ok=True)\nos.makedirs(args.log_dir, exist_ok=True)\nos.environ['CUDA_VISIBLE_DEVICES'] = args.CUDA_VISIBLE_DEVICES\n\nif args.enet_type == 'resnest101':\n    ModelClass = Resnest_Melanoma\nelif args.enet_type == 'seresnext101':\n    ModelClass = Seresnext_Melanoma\nelif 'efficientnet' in args.enet_type:\n    ModelClass = Effnet_Melanoma\nelse:\n    raise NotImplementedError()\n\nDP = len(os.environ['CUDA_VISIBLE_DEVICES']) > 1\n\nset_seed()\n\ncriterion = nn.CrossEntropyLoss()\n\ntrain()\n","metadata":{"execution":{"iopub.status.busy":"2023-03-15T12:44:48.768254Z","iopub.execute_input":"2023-03-15T12:44:48.769161Z","iopub.status.idle":"2023-03-15T22:36:05.678528Z","shell.execute_reply.started":"2023-03-15T12:44:48.769126Z","shell.execute_reply":"2023-03-15T22:36:05.677496Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/albumentations/augmentations/transforms.py:1151: FutureWarning: This class has been deprecated. Please use RandomBrightnessContrast\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/albumentations/augmentations/transforms.py:1177: FutureWarning: RandomContrast has been deprecated. Please use RandomBrightnessContrast\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/albumentations/augmentations/blur/transforms.py:185: UserWarning: blur_limit and sigma_limit minimum value can not be both equal to 0. blur_limit minimum value changed to 3.\n  \"blur_limit and sigma_limit minimum value can not be both equal to 0. \"\n/opt/conda/lib/python3.7/site-packages/albumentations/augmentations/dropout/cutout.py:51: FutureWarning: Cutout has been deprecated. Please use CoarseDropout\n  FutureWarning,\n","output_type":"stream"},{"name":"stdout","text":"Data preprocessed\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/efficientnet_b3_ra2-cf984f9c.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b3_ra2-cf984f9c.pth\n","output_type":"stream"},{"name":"stdout","text":"46382 11582\nWed Mar 15 12:44:53 2023 Fold 0, Epoch 1\n","output_type":"stream"},{"name":"stderr","text":"loss: 0.66705, smth: 0.81273: 100%|██████████| 725/725 [20:24<00:00,  1.69s/it]\n100%|██████████| 181/181 [01:30<00:00,  2.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Wed Mar 15 13:06:48 2023 Fold 0, Epoch 1, lr: 0.0000300, train loss: 1.06353, valid loss: 0.70422, acc: 75.7296, auc: 0.881216, auc_20: 0.803660.\nauc_max (0.000000 --> 0.881216). Saving model ...\nauc_20_max (0.000000 --> 0.803660). Saving model ...\nWed Mar 15 13:06:48 2023 Fold 0, Epoch 2\n","output_type":"stream"},{"name":"stderr","text":"loss: 0.70866, smth: 0.62320: 100%|██████████| 725/725 [19:02<00:00,  1.58s/it]\n100%|██████████| 181/181 [00:54<00:00,  3.34it/s]\n/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:1360: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n  \"please use `get_last_lr()`.\", UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"Wed Mar 15 13:26:44 2023 Fold 0, Epoch 2, lr: 0.0003000, train loss: 0.69818, valid loss: 0.51127, acc: 82.6369, auc: 0.912784, auc_20: 0.857350.\nauc_max (0.881216 --> 0.912784). Saving model ...\nauc_20_max (0.803660 --> 0.857350). Saving model ...\nWed Mar 15 13:26:45 2023 Fold 0, Epoch 3\n","output_type":"stream"},{"name":"stderr","text":"loss: 0.54739, smth: 0.57382: 100%|██████████| 725/725 [19:42<00:00,  1.63s/it]\n100%|██████████| 181/181 [00:53<00:00,  3.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Wed Mar 15 13:47:21 2023 Fold 0, Epoch 3, lr: 0.0002962, train loss: 0.58248, valid loss: 0.46856, acc: 83.5693, auc: 0.918748, auc_20: 0.868820.\nauc_max (0.912784 --> 0.918748). Saving model ...\nauc_20_max (0.857350 --> 0.868820). Saving model ...\nWed Mar 15 13:47:21 2023 Fold 0, Epoch 4\n","output_type":"stream"},{"name":"stderr","text":"loss: 0.75947, smth: 0.51613: 100%|██████████| 725/725 [19:34<00:00,  1.62s/it]\n100%|██████████| 181/181 [00:55<00:00,  3.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Wed Mar 15 14:07:51 2023 Fold 0, Epoch 4, lr: 0.0002851, train loss: 0.52767, valid loss: 0.44282, acc: 84.5882, auc: 0.926274, auc_20: 0.877191.\nauc_max (0.918748 --> 0.926274). Saving model ...\nauc_20_max (0.868820 --> 0.877191). Saving model ...\nWed Mar 15 14:07:51 2023 Fold 0, Epoch 5\n","output_type":"stream"},{"name":"stderr","text":"loss: 0.48502, smth: 0.48691: 100%|██████████| 725/725 [19:23<00:00,  1.61s/it]\n100%|██████████| 181/181 [00:54<00:00,  3.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Wed Mar 15 14:28:09 2023 Fold 0, Epoch 5, lr: 0.0002673, train loss: 0.48986, valid loss: 0.41799, acc: 85.4775, auc: 0.933030, auc_20: 0.863656.\nauc_max (0.926274 --> 0.933030). Saving model ...\nWed Mar 15 14:28:10 2023 Fold 0, Epoch 6\n","output_type":"stream"},{"name":"stderr","text":"loss: 0.33013, smth: 0.45316: 100%|██████████| 725/725 [19:20<00:00,  1.60s/it]\n100%|██████████| 181/181 [00:54<00:00,  3.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Wed Mar 15 14:48:25 2023 Fold 0, Epoch 6, lr: 0.0002435, train loss: 0.45441, valid loss: 0.41206, acc: 85.8660, auc: 0.932559, auc_20: 0.876916.\nWed Mar 15 14:48:25 2023 Fold 0, Epoch 7\n","output_type":"stream"},{"name":"stderr","text":"loss: 0.35671, smth: 0.42794: 100%|██████████| 725/725 [19:28<00:00,  1.61s/it]\n100%|██████████| 181/181 [00:54<00:00,  3.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Wed Mar 15 15:08:48 2023 Fold 0, Epoch 7, lr: 0.0002151, train loss: 0.42227, valid loss: 0.37357, acc: 87.3079, auc: 0.946397, auc_20: 0.916500.\nauc_max (0.933030 --> 0.946397). Saving model ...\nauc_20_max (0.877191 --> 0.916500). Saving model ...\nWed Mar 15 15:08:48 2023 Fold 0, Epoch 8\n","output_type":"stream"},{"name":"stderr","text":"loss: 0.37711, smth: 0.40276: 100%|██████████| 725/725 [19:33<00:00,  1.62s/it]\n100%|██████████| 181/181 [00:53<00:00,  3.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Wed Mar 15 15:29:15 2023 Fold 0, Epoch 8, lr: 0.0001834, train loss: 0.40012, valid loss: 0.37413, acc: 87.7223, auc: 0.945411, auc_20: 0.910430.\nWed Mar 15 15:29:15 2023 Fold 0, Epoch 9\n","output_type":"stream"},{"name":"stderr","text":"loss: 0.71358, smth: 0.37985: 100%|██████████| 725/725 [19:22<00:00,  1.60s/it]\n100%|██████████| 181/181 [00:55<00:00,  3.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Wed Mar 15 15:49:33 2023 Fold 0, Epoch 9, lr: 0.0001500, train loss: 0.36476, valid loss: 0.36016, acc: 87.8864, auc: 0.946752, auc_20: 0.884873.\nauc_max (0.946397 --> 0.946752). Saving model ...\nWed Mar 15 15:49:34 2023 Fold 0, Epoch 10\n","output_type":"stream"},{"name":"stderr","text":"loss: 0.30927, smth: 0.33476: 100%|██████████| 725/725 [18:56<00:00,  1.57s/it]\n100%|██████████| 181/181 [00:52<00:00,  3.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Wed Mar 15 16:09:23 2023 Fold 0, Epoch 10, lr: 0.0001166, train loss: 0.34030, valid loss: 0.34920, acc: 88.2058, auc: 0.954842, auc_20: 0.927269.\nauc_max (0.946752 --> 0.954842). Saving model ...\nauc_20_max (0.916500 --> 0.927269). Saving model ...\nWed Mar 15 16:09:23 2023 Fold 0, Epoch 11\n","output_type":"stream"},{"name":"stderr","text":"loss: 0.49532, smth: 0.31693: 100%|██████████| 725/725 [18:19<00:00,  1.52s/it]\n100%|██████████| 181/181 [00:52<00:00,  3.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Wed Mar 15 16:28:35 2023 Fold 0, Epoch 11, lr: 0.0000849, train loss: 0.31961, valid loss: 0.34145, acc: 88.6721, auc: 0.955236, auc_20: 0.923619.\nauc_max (0.954842 --> 0.955236). Saving model ...\nWed Mar 15 16:28:36 2023 Fold 0, Epoch 12\n","output_type":"stream"},{"name":"stderr","text":"loss: 0.22524, smth: 0.29068: 100%|██████████| 725/725 [18:48<00:00,  1.56s/it]\n100%|██████████| 181/181 [00:58<00:00,  3.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Wed Mar 15 16:48:22 2023 Fold 0, Epoch 12, lr: 0.0000565, train loss: 0.30054, valid loss: 0.33672, acc: 88.8966, auc: 0.955614, auc_20: 0.923322.\nauc_max (0.955236 --> 0.955614). Saving model ...\nWed Mar 15 16:48:23 2023 Fold 0, Epoch 13\n","output_type":"stream"},{"name":"stderr","text":"loss: 0.32909, smth: 0.29206: 100%|██████████| 725/725 [18:23<00:00,  1.52s/it]\n100%|██████████| 181/181 [00:51<00:00,  3.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Wed Mar 15 17:07:38 2023 Fold 0, Epoch 13, lr: 0.0000327, train loss: 0.28261, valid loss: 0.34170, acc: 88.9915, auc: 0.955357, auc_20: 0.921506.\nWed Mar 15 17:07:38 2023 Fold 0, Epoch 14\n","output_type":"stream"},{"name":"stderr","text":"loss: 0.41890, smth: 0.27899: 100%|██████████| 725/725 [18:13<00:00,  1.51s/it]\n100%|██████████| 181/181 [00:51<00:00,  3.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Wed Mar 15 17:26:43 2023 Fold 0, Epoch 14, lr: 0.0000149, train loss: 0.27273, valid loss: 0.33941, acc: 89.2592, auc: 0.955494, auc_20: 0.926536.\nWed Mar 15 17:26:43 2023 Fold 0, Epoch 15\n","output_type":"stream"},{"name":"stderr","text":"loss: 0.48835, smth: 0.26047: 100%|██████████| 725/725 [18:07<00:00,  1.50s/it]\n100%|██████████| 181/181 [00:51<00:00,  3.52it/s]\n","output_type":"stream"},{"name":"stdout","text":"Wed Mar 15 17:45:42 2023 Fold 0, Epoch 15, lr: 0.0000038, train loss: 0.26971, valid loss: 0.34020, acc: 89.1988, auc: 0.956064, auc_20: 0.927569.\nauc_max (0.955614 --> 0.956064). Saving model ...\nauc_20_max (0.927269 --> 0.927569). Saving model ...\n46375 11589\nWed Mar 15 17:45:42 2023 Fold 1, Epoch 1\n","output_type":"stream"},{"name":"stderr","text":"loss: 0.99125, smth: 0.79779: 100%|██████████| 725/725 [17:54<00:00,  1.48s/it]\n100%|██████████| 182/182 [00:50<00:00,  3.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"Wed Mar 15 18:04:27 2023 Fold 1, Epoch 1, lr: 0.0000300, train loss: 1.07489, valid loss: 0.69327, acc: 76.7193, auc: 0.895466, auc_20: 0.858423.\nauc_max (0.000000 --> 0.895466). Saving model ...\nauc_20_max (0.000000 --> 0.858423). Saving model ...\nWed Mar 15 18:04:27 2023 Fold 1, Epoch 2\n","output_type":"stream"},{"name":"stderr","text":"loss: 0.60379, smth: 0.63581: 100%|██████████| 725/725 [17:57<00:00,  1.49s/it]\n100%|██████████| 182/182 [00:50<00:00,  3.61it/s]\n/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:1360: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n  \"please use `get_last_lr()`.\", UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"Wed Mar 15 18:23:16 2023 Fold 1, Epoch 2, lr: 0.0003000, train loss: 0.69987, valid loss: 0.51692, acc: 82.2936, auc: 0.921878, auc_20: 0.867858.\nauc_max (0.895466 --> 0.921878). Saving model ...\nauc_20_max (0.858423 --> 0.867858). Saving model ...\nWed Mar 15 18:23:16 2023 Fold 1, Epoch 3\n","output_type":"stream"},{"name":"stderr","text":"loss: 0.49308, smth: 0.59135: 100%|██████████| 725/725 [17:54<00:00,  1.48s/it]\n100%|██████████| 182/182 [00:50<00:00,  3.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"Wed Mar 15 18:42:01 2023 Fold 1, Epoch 3, lr: 0.0002962, train loss: 0.59014, valid loss: 0.45006, acc: 84.5457, auc: 0.929788, auc_20: 0.887495.\nauc_max (0.921878 --> 0.929788). Saving model ...\nauc_20_max (0.867858 --> 0.887495). Saving model ...\nWed Mar 15 18:42:02 2023 Fold 1, Epoch 4\n","output_type":"stream"},{"name":"stderr","text":"loss: 0.28013, smth: 0.50393: 100%|██████████| 725/725 [18:20<00:00,  1.52s/it]\n100%|██████████| 182/182 [00:51<00:00,  3.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Wed Mar 15 19:01:15 2023 Fold 1, Epoch 4, lr: 0.0002851, train loss: 0.53627, valid loss: 0.42529, acc: 85.2791, auc: 0.937073, auc_20: 0.904589.\nauc_max (0.929788 --> 0.937073). Saving model ...\nauc_20_max (0.887495 --> 0.904589). Saving model ...\nWed Mar 15 19:01:15 2023 Fold 1, Epoch 5\n","output_type":"stream"},{"name":"stderr","text":"loss: 0.71098, smth: 0.50118: 100%|██████████| 725/725 [18:24<00:00,  1.52s/it]\n100%|██████████| 182/182 [00:52<00:00,  3.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Wed Mar 15 19:20:32 2023 Fold 1, Epoch 5, lr: 0.0002673, train loss: 0.49450, valid loss: 0.42044, acc: 85.8486, auc: 0.937146, auc_20: 0.892981.\nauc_max (0.937073 --> 0.937146). Saving model ...\nWed Mar 15 19:20:32 2023 Fold 1, Epoch 6\n","output_type":"stream"},{"name":"stderr","text":"loss: 0.48072, smth: 0.43980: 100%|██████████| 725/725 [18:46<00:00,  1.55s/it]\n100%|██████████| 182/182 [00:52<00:00,  3.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Wed Mar 15 19:40:11 2023 Fold 1, Epoch 6, lr: 0.0002435, train loss: 0.45868, valid loss: 0.40307, acc: 86.5217, auc: 0.935656, auc_20: 0.886818.\nWed Mar 15 19:40:11 2023 Fold 1, Epoch 7\n","output_type":"stream"},{"name":"stderr","text":"loss: 0.42430, smth: 0.42572: 100%|██████████| 725/725 [19:25<00:00,  1.61s/it]\n100%|██████████| 182/182 [00:55<00:00,  3.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Wed Mar 15 20:00:31 2023 Fold 1, Epoch 7, lr: 0.0002151, train loss: 0.42859, valid loss: 0.39079, acc: 86.6770, auc: 0.944628, auc_20: 0.898706.\nauc_max (0.937146 --> 0.944628). Saving model ...\nWed Mar 15 20:00:32 2023 Fold 1, Epoch 8\n","output_type":"stream"},{"name":"stderr","text":"loss: 0.43168, smth: 0.38278: 100%|██████████| 725/725 [19:03<00:00,  1.58s/it]\n100%|██████████| 182/182 [00:52<00:00,  3.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Wed Mar 15 20:20:27 2023 Fold 1, Epoch 8, lr: 0.0001834, train loss: 0.40033, valid loss: 0.37526, acc: 86.9790, auc: 0.950256, auc_20: 0.925348.\nauc_max (0.944628 --> 0.950256). Saving model ...\nauc_20_max (0.904589 --> 0.925348). Saving model ...\nWed Mar 15 20:20:28 2023 Fold 1, Epoch 9\n","output_type":"stream"},{"name":"stderr","text":"loss: 0.21303, smth: 0.37093: 100%|██████████| 725/725 [18:17<00:00,  1.51s/it]\n100%|██████████| 182/182 [00:52<00:00,  3.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Wed Mar 15 20:39:38 2023 Fold 1, Epoch 9, lr: 0.0001500, train loss: 0.37489, valid loss: 0.36374, acc: 87.3846, auc: 0.955088, auc_20: 0.926217.\nauc_max (0.950256 --> 0.955088). Saving model ...\nauc_20_max (0.925348 --> 0.926217). Saving model ...\nWed Mar 15 20:39:38 2023 Fold 1, Epoch 10\n","output_type":"stream"},{"name":"stderr","text":"loss: 0.24840, smth: 0.34815: 100%|██████████| 725/725 [18:24<00:00,  1.52s/it]\n100%|██████████| 182/182 [00:52<00:00,  3.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Wed Mar 15 20:58:55 2023 Fold 1, Epoch 10, lr: 0.0001166, train loss: 0.34500, valid loss: 0.36688, acc: 87.7729, auc: 0.953864, auc_20: 0.919369.\nWed Mar 15 20:58:55 2023 Fold 1, Epoch 11\n","output_type":"stream"},{"name":"stderr","text":"loss: 0.24626, smth: 0.30719: 100%|██████████| 725/725 [18:38<00:00,  1.54s/it]\n100%|██████████| 182/182 [00:52<00:00,  3.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Wed Mar 15 21:18:27 2023 Fold 1, Epoch 11, lr: 0.0000849, train loss: 0.32033, valid loss: 0.35115, acc: 88.4373, auc: 0.957738, auc_20: 0.929471.\nauc_max (0.955088 --> 0.957738). Saving model ...\nauc_20_max (0.926217 --> 0.929471). Saving model ...\nWed Mar 15 21:18:27 2023 Fold 1, Epoch 12\n","output_type":"stream"},{"name":"stderr","text":"loss: 0.25202, smth: 0.30858: 100%|██████████| 725/725 [18:41<00:00,  1.55s/it]\n100%|██████████| 182/182 [00:53<00:00,  3.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Wed Mar 15 21:38:02 2023 Fold 1, Epoch 12, lr: 0.0000565, train loss: 0.29902, valid loss: 0.34991, acc: 88.7652, auc: 0.953984, auc_20: 0.917433.\nWed Mar 15 21:38:02 2023 Fold 1, Epoch 13\n","output_type":"stream"},{"name":"stderr","text":"loss: 0.30464, smth: 0.27845: 100%|██████████| 725/725 [18:30<00:00,  1.53s/it]\n100%|██████████| 182/182 [00:52<00:00,  3.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Wed Mar 15 21:57:25 2023 Fold 1, Epoch 13, lr: 0.0000327, train loss: 0.28558, valid loss: 0.34483, acc: 88.9464, auc: 0.958331, auc_20: 0.922056.\nauc_max (0.957738 --> 0.958331). Saving model ...\nWed Mar 15 21:57:25 2023 Fold 1, Epoch 14\n","output_type":"stream"},{"name":"stderr","text":"loss: 0.35212, smth: 0.27179: 100%|██████████| 725/725 [18:23<00:00,  1.52s/it]\n100%|██████████| 182/182 [00:52<00:00,  3.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Wed Mar 15 22:16:40 2023 Fold 1, Epoch 14, lr: 0.0000149, train loss: 0.27445, valid loss: 0.34463, acc: 89.1880, auc: 0.957855, auc_20: 0.924843.\nWed Mar 15 22:16:40 2023 Fold 1, Epoch 15\n","output_type":"stream"},{"name":"stderr","text":"loss: 0.38328, smth: 0.26539: 100%|██████████| 725/725 [18:32<00:00,  1.53s/it]\n100%|██████████| 182/182 [00:52<00:00,  3.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Wed Mar 15 22:36:05 2023 Fold 1, Epoch 15, lr: 0.0000038, train loss: 0.27256, valid loss: 0.34145, acc: 89.2398, auc: 0.958082, auc_20: 0.922443.\n","output_type":"stream"}]},{"cell_type":"code","source":"args = Args('predict')\nargs.use_meta = False\nargs.model_dir = './Ensemble-Deep-Learning-Melanoma-Competition-Pytorch/weights/weights_nometa'\nargs.sub_dir = './Ensemble-Deep-Learning-Melanoma-Competition-Pytorch/subs/subs_nometa'\n\nos.makedirs(args.sub_dir, exist_ok=True)\nos.environ['CUDA_VISIBLE_DEVICES'] = args.CUDA_VISIBLE_DEVICES\n\nif args.enet_type == 'resnest101':\n    ModelClass = Resnest_Melanoma\nelif args.enet_type == 'seresnext101':\n    ModelClass = Seresnext_Melanoma\nelif 'efficientnet' in args.enet_type:\n    ModelClass = Effnet_Melanoma\nelse:\n    raise NotImplementedError()\n\nDP = len(os.environ['CUDA_VISIBLE_DEVICES']) > 1\n\npredict()    ","metadata":{"execution":{"iopub.status.busy":"2023-03-15T22:40:50.760155Z","iopub.execute_input":"2023-03-15T22:40:50.761217Z","iopub.status.idle":"2023-03-15T22:46:36.449873Z","shell.execute_reply.started":"2023-03-15T22:40:50.761171Z","shell.execute_reply":"2023-03-15T22:46:36.445536Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"100%|██████████| 172/172 [05:44<00:00,  2.00s/it]\n","output_type":"stream"}]}]}